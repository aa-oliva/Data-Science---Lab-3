# -*- coding: utf-8 -*-
"""FinalLab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJRFbXSv_hEn8FOgQ0rX71FvttLXPfgP

# Laboratorio 4

---
Se necesita tener acceso los archivos de google drive para poder usar este código, aqui esta el link de compartir: <br>[Google Drive](https://drive.google.com/drive/folders/1EGaVv-qekeA--pPsToN5DUZC868gv14k?usp=sharing) <br>

--- 
Tabla de contenido


*   Análisis Exploratorio
*   something else

## IMPORTACIONES
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from google.colab import files
import matplotlib.pyplot as plt
import re
import io 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from nltk import ngrams
import heapq
import operator
# Load library
from nltk.corpus import stopwords
import os
# You will have to download the set of stop words the first time
import nltk
nltk.download('stopwords')

"""### Conexión a Google Drive"""

!pip install -U -q PyDrive ## you will have install for every colab session
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_list = drive.ListFile({'q': "'1ki9mkoQzQUBeqcUIzud04eAglroryOA7' in parents and trashed=false"}).GetList()
for file1 in file_list:
  print('title: %s, id: %s' % (file1['title'], file1['id']))

"""### Descarga de los archivos
se consiguen los archivos de google drive, y se van a utilizar los mimos nombres que tenían para llamarlos
"""

enUSNews = drive.CreateFile({'id': '1fUYXYLPfasmPrKfeB2sb3VjEQeay3Afe'})
enUSNews.GetContentFile('en_US.news.txt')

enUSBlogs = drive.CreateFile({'id':'1W2rALeghgra2E6h8t9Pu9dqnFHkIX3lg'})
enUSBlogs.GetContentFile('en_US.blogs.txt')

enUSTwitter = drive.CreateFile({'id':'1-TOVJ4t54YYln8-odlTUxH_dSJTqs92s'})
enUSTwitter.GetContentFile('en_US.twitter.txt')

preposiciones = drive.CreateFile({'id': '1zlbTQdhuo2UKDB5V1B_VGfzbCqB5tQpC'})
preposiciones.GetContentFile('prepositions.txt')

conjunctions = drive.CreateFile({'id': '1ZbsphfT4RbCzgCaOqEhjZXTzl-eFu3rn'})
conjunctions.GetContentFile('conjunctions.txt')

def file_len(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1

"""### Cantidad de lineas en los archivos"""

lineasNews = file_len('en_US.news.txt')
lineasBlogs = file_len('en_US.blogs.txt')
lineasTwitter = file_len('en_US.twitter.txt')

"""### 5% de esas lineas"""

def reducirTexto(file, lineas):
  lineas = int(round(lineas * 0.05))
  with open(file) as myfile:
      head = [next(myfile) for x in range(lineas)]
  return head

def mkdir(name, data):
  f = open(name, 'w')
  f.write(str(data))
  f.close()

def openF(filename):
  file = open(filename, 'r')
  return file.read()

reduccion5porciento = reducirTexto('en_US.news.txt', lineasNews)
mkdir("en_US.news.txt", reduccion5porciento)
reduccion5porciento = reducirTexto('en_US.blogs.txt', lineasBlogs)
mkdir("en_US.blogs.txt", reduccion5porciento)
reduccion5porciento = reducirTexto('en_US.twitter.txt', lineasTwitter)
mkdir("en_US.twitter.txt", reduccion5porciento)

"""## Limpieza de datos (remover @,# y apostrofes)

- Revisar si hay emoticones y quitarlos
- Quitar los artículos, preposiciones y conjunciones (stopwords)
- Quitar números si considera que interferirán en las predicciones.
- Se van a eliminar los emojis, leyendolos por hex y identificando los que estan llamando a emojis
- Quitar los stopwords utilizando NLKT
- Se van a guardar los archivos limpios en:
    * news.txt
    *  blogs.txt
    * twitter.txt
"""

def lowerYsub(filename, fileout):
  with open(filename, 'r') as infile, \
       open(fileout, 'w') as outfile:
      data = infile.read()
      data = data.lower()
     
      # Eliminar simbolos
      data = re.sub("@", "", data)
      data = re.sub("#", "", data)
      data = re.sub("&", "", data)
      data = re.sub('\"','', data)      
      data = re.sub("'",'', data)
      data = re.sub(" n, ",'', data)
      data = re.sub(" n ",'', data)
      data = re.sub(" s ",'', data)
      data = re.sub('\,',' ', data)
      data = re.sub('\.',' ', data)
      
      #Eliminar enlaces
      data = re.sub(r'\bhttp\S+', '', data, re.IGNORECASE)
      
      # Eliminar Articulos
      data = data.replace(' a ', ' ')
      data = data.replace(" n ",' ')
      data = data.replace(" s ",' ')
      data = data.replace(" an ", " ")
      data = data.replace(" the ", " ")
      data = data.replace(" and ", " ")
      data = data.replace("\n", " ")
      data = data.replace("\\", " ")
      data = data.replace(".", " ")

      
      outfile.write(data)
      print ("Success")

lowerYsub('en_US.news.txt','news.txt')
lowerYsub('en_US.blogs.txt','blogs.txt')
lowerYsub('en_US.twitter.txt','twitter.txt')

"""##Quitando Emojis"""

from codecs import open
try:
    from contextlib import nested  # Python 2
except ImportError:
    from contextlib import ExitStack, contextmanager

    @contextmanager
    def nested(*contexts):
        """
        Reimplementation of nested in python 3.
        """
        with ExitStack() as stack:
            for ctx in contexts:
                stack.enter_context(ctx)
            yield contexts
import re
import sys
from os import path


EMOJI_RE = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251" 
                           "]+", flags=re.UNICODE)

def uopen(*args):
    return open(*args, encoding="UTF-8")

def remove_emoji(in_fname, out_fname):
    with nested(uopen(in_fname), uopen(out_fname, "w")) as (inf, outf):
        for line in inf:
            outf.write(EMOJI_RE.sub(r'', line))

def main():
    in_fname = 'twitter.txt'
    out_fname = 'twitter2.txt'
    in2_fname = 'blogs.txt'
    out2_fname = 'blogs2.txt'
    in3_fname = 'news.txt'
    out3_fname = 'news2.txt'

    remove_emoji(in_fname, out_fname)
    remove_emoji(in2_fname, out2_fname)
    remove_emoji(in3_fname, out3_fname)
    
    
if __name__ == "__main__":
    main()

"""##Quitando Stop Words"""

import io 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english')) 


def StopWordRemover2000(filename, filewrite):
  file1 = open(filename ,encoding="UTF-8") 
  line = file1.read()# Use this to read file content as a stream: 
  words = line.split() 
  for r in words: 
      if not r in stop_words: 
          appendFile = open(filewrite,'a',encoding="UTF-8") 
          appendFile.write(" "+r) 
          appendFile.close()
  print('Success')

StopWordRemover2000("news2.txt", "news.txt")
StopWordRemover2000("blogs2.txt", "blogs.txt")
StopWordRemover2000("twitter2.txt", "twitter.txt")

"""## Analisis

## Diccionario de Palabras
Aqui se ba a buscar hacer un contador de cuantas palabras únicas hay y cuantas veces se repite ciertas palabras. Esto es para sacar la frequencia de las palabras y poder sacar graficas del resultado
"""

palabrasNews = {}
palabrasBlog = {}
palabrasTwitter = {}

def word_count(str):
    counts = dict()
    words = str.split()
    for word in words:
        if word in counts:
            counts[word] += 1
        else:
            counts[word] = 1
    return counts

with open('news.txt', 'r') as infile:
  data = infile.read()
  palabrasNews = word_count(data)

with open('blogs.txt', 'r') as infile:
  data = infile.read()
  palabrasBlog = word_count(data)

with open('twitter.txt', 'r') as infile:
  data = infile.read()
  palabrasTwitter = word_count(data)

"""### Ordenar listado
Lo ordenamos para poder hacer busquedas mas rapidas y encontrar las palabras mas comunes.
"""

sorted(palabrasNews.items(), key=operator.itemgetter(1), reverse=True)

"""##Histogramas

Se realizan histogramas para un mejor entendimiento
"""

def keyInOrder(listName, number):
  keys = list(sorted(listName, key=listName.__getitem__, reverse=True))
  keys = keys[:number]
  firstFew = {x:listName[x] for x in keys}
  return firstFew

"""### News

Podemos observar que la palabra que mas se repite en News es n seguida por to y of
"""

keys = keyInOrder(palabrasNews, 20)

plt.bar(keys.keys(), keys.values(), color='b')
plt.title("News")

"""### Blogs

Podemos observar que la palabra que mas se repite en Blogs es n
"""

keys = keyInOrder(palabrasBlog, 20)

plt.bar(keys.keys(), keys.values(), color='b')
plt.title("Blog")

"""### Twitter

Podemos observar que la palabra que mas se repite en Twitter es n, seguida por i y to
"""

keys = keyInOrder(palabrasTwitter, 15)

plt.bar(keys.keys(), keys.values(), color='b')
plt.title("Twitter")

"""##N-Grams

Los N-gramas se generan dividiendo el corpus de cada archivo en tokens, y se acoplan por N componentes.
Por ejemplo se divie en bigrama "n = 2", se acopla por cada 2 palabras.

Para este proceso se utilizara la libreria nltk con el apartado ngrams
###ngrams(corpus.tokens, n)

###Ejemplo:

Corpus : "I really like python, it's pretty awesome."

n = 4

output:

['I', 'really', 'like', 'python,']

['really', 'like', 'python,', "it's"]

['like', 'python,', "it's", 'pretty']

['python,', "it's", 'pretty', 'awesome.']
"""

def Ngram(n, text, printing):
  limiter = 0
  nGram = ngrams(text.split(), n)
  if (printing):
    for grams in nGram:
      if (limiter > 50):
         break
      print (grams)    
      limiter += 1
  return nGram

news = openF("news.txt")
blogs = openF("blogs.txt")
twitter = openF("twitter.txt")

biGramNews = Ngram(2, news, True)
triGramNew = Ngram(3, news, True)

biGramblogs = Ngram(2, blogs, True)
triGramBlogs = Ngram(3, blogs, True)

biGramTwitter = Ngram(2, twitter, True)
triGramTwitter = Ngram(3, twitter, True)

"""##Matrices de términos para cada palabra

En este apartado demostramos la frecuencia de cada anagrama, y asi poder sacar la probabilidad de cada anagrama resultado dado las probabilidades de ***palabra1*** dado ***palabra2***
"""

def tMatrix(nGram, printiing):
  fdist = nltk.FreqDist(nGram)
  limiter = 0
  for k, v in fdist.items():
    print (k,v)
    limiter += 1
    if (limiter > 50):
      break
#   return fdist;

tMatrix(biGramNews)

tMatrix(triGramNew)

tMatrix(biGramblogs)

tMatrix(triGramBlogs)

tMatrix(biGramTwitter)

tMatrix(triGramTwitter)

"""## Probabilidad de ocurrencia de cada uno de los n-gramas

## Algoritmo de Prediccion
Se utilizo el algoritmo de KneserNey que recore los anagramas con el input dado para ver la frequencia de las palabras que aparecen despues del input y compara las frequencias para dar un porcentaje de probabilidad en la cual va a aparecer la siguiente palabra.

KneserNeyProbDist devuelve la probabilidad de la siguiente palabra dado  las palabras anteriores, como ejemplo se utilizaran bigramas.


##Ejemplo

Utilizando las palabras:
###palabra 1: I
###Palabra 2: confess


(u'I', u'confess', u'.--'):0.00657894736842

(u'I', u'confess', u'what'):0.00657894736842   

(u'I', u'confess', u'myself'):0.00657894736842

(u'I', u'confess', u'also'):0.00657894736842

(u'I', u'confess', u'there'):0.00657894736842

(u'I', u'confess', u',"'):0.0328947368421

(u'I', u'confess', u'that'):0.164473684211

(u'I', u'confess', u'"--'):0.00657894736842

(u'I', u'confess', u'it'):0.0328947368421

(u'I', u'confess', u';'):0.00657894736842

(u'I', u'confess', u','):0.269736842105

(u'I', u'confess', u'I'):0.164473684211

(u'I', u'confess', u'unto'):0.00657894736842

(u'I', u'confess', u'is'):0.00657894736842


0.723684210526


podemos identificar que la siguiente palabra sera "**I**" o el siguiente caracter sera "**,**"
"""

# Funcion que saca la probabilidad de la siguiente palabra de un bigrama
def biKneserNey(nGram, palabra1, palabra2):
  fdist = nltk.FreqDist(nGram)
  kneser_ney = nltk.KneserNeyProbDist(fdist)
  prob_sum = 0 
  dicPalabras3 = {}
  
  for i in kneser_ney.samples():
    
    if i[0] == palabra1 and i[1] == palabra2:
      dicPalabras3[i[2]] = kneser_ney.prob(i)
#       print ("{0}:{1}".format(i, kneser_ney.prob(i)))
  if (dicPalabras3 == {}):
    return '[END] no mas iteraciones'
  return sorted(dicPalabras3.items(), key=operator.itemgetter(1), reverse=True)[:1][0][0]

# Funcion que saca la probabilidad de la siguiente palabra de un bigrama
def biKneserNeyBackup(nGram, palabra1, palabra2):
  fdist = nltk.FreqDist(nGram)
  kneser_ney = nltk.KneserNeyProbDist(fdist)
  prob_sum = 0 
  limiter = 0
  
  for i in kneser_ney.samples():
    if i[0] == palabra1 and i[1] == palabra2:
      prob_sum += kneser_ney.prob(i)
      print ("{0}:{1}".format(i, kneser_ney.prob(i)))
    print (prob_sum)
    limiter += 1
    if (limiter > 50):
      break
#   return kneser_ney.max()

#funcion que saca la probabilidad de la siguiente palabra en un trigrama
def triKneserNey(nGram, palabra1, palabra2, palabra3):
  fdist = nltk.FreqDist(nGram)
  kneser_ney = nltk.KneserNeyProbDist(fdist)

  prob_sum = 0 
  limiter = 0
   
  for i in kneser_ney.samples():
    if [0] == palabra1 and i[1] == palabra2 and i[2] == palabra3:
      prob_sum += kneser_ney.prob(i)
      print ("{0}:{1}:{2}".format(i, kneser_ney.prob(i)))
    print (prob_sum)
    limiter += 1
    if (limiter > 50):
      break
  return kneser_ney.max()

"""## Prediccion de 3 palabras siguientes"""

# se crean las funciones de prediccion para bigramaso trigramas, dependiendo la 
#cantidad de palabras que ingfrese el usuario

def biprediccion(nGrama, palabra1, palabra2):
  palabraTemp = biKneserNey(nGrama, palabra1, palabra2)
  palabra1 = palabra2
  palabra2 = str(palabraTemp)

#  biprediccion(nGrama, )
  print("La siguiente palabra es: ", palabra2)
  #print (palabra1)
  return palabra1, palabra2
    
def triPrediccion(palara1, palabra2, palabra3):
  
  triKneserNey(nGrama, palabra1, palabra2, palabra3)
  palabra1 = palabra2
  palabra2 = palabra3
#   palabra3 = sigPalabraTri()
    
  print("La siguiente palabra es: ", palabra3)

def predicccionFinal(palabra1, palabra2):
  for x in range(3):
    triGramNew = Ngram(3, news, False)
    palabra1, palabra2 = biprediccion(triGramNew, palabra1, palabra2)

"""## Prediccion con Input
En esta parte se puede ingresar dos palabras para que busque cuales sea las 3 palabra mas probable que va a salir despues de esas dos. Esto se logra agregando la palabra encontrada en el primer recorido y guardandolo como el nuevo input, asi el programa puede buscar la palabra despues de la encontrada
"""

solucion = input("Ingrese dos palabras: ")
print (' -- Working... ---')
dospalabras = solucion.split(' ')
predicccionFinal(dospalabras[0],dospalabras[1])